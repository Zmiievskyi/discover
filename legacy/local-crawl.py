import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import json
import random
import sqlite3
from datetime import datetime

try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False
    print("‚ö†Ô∏è ChromaDB –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –î–ª—è AI-–ø–æ–∏—Å–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install chromadb")


class ChromaVectorDB:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å ChromaDB (–≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–ª—è AI-–ø–æ–∏—Å–∫–∞)"""

    def __init__(self, collection_name='crawled_pages', persist_directory='./chroma_db'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB

        Args:
            collection_name: –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è embeddings
            persist_directory: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        """
        if not CHROMADB_AVAILABLE:
            raise ImportError("ChromaDB –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install chromadb")

        self.client = chromadb.PersistentClient(path=persist_directory)

        # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
        try:
            self.collection = self.client.get_collection(name=collection_name)
            print(f"‚úì ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è '{collection_name}' –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except:
            self.collection = self.client.create_collection(
                name=collection_name,
                metadata={"description": "Web crawling results with semantic search"}
            )
            print(f"‚úì ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è '{collection_name}' —Å–æ–∑–¥–∞–Ω–∞")

    def add_page(self, url, title, content, metadata=None):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É

        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ ID)
            title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            content: –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        """
        try:
            # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (title + content)
            text_for_embedding = f"{title}\n\n{content}"

            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            meta = {
                'url': url,
                'title': title,
                'content_length': len(content)
            }
            if metadata:
                meta.update(metadata)

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ ChromaDB
            # ChromaDB –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞—Å—Ç embedding –∏—Å–ø–æ–ª—å–∑—É—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            self.collection.add(
                documents=[text_for_embedding],
                metadatas=[meta],
                ids=[url]  # URL –∫–∞–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
            )
            return True
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ ChromaDB: {e}")
            return False

    def search(self, query, n_results=5):
        """
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É

        Args:
            query: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results
            )

            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            formatted_results = []
            if results['ids'] and len(results['ids'][0]) > 0:
                for i in range(len(results['ids'][0])):
                    formatted_results.append({
                        'url': results['ids'][0][i],
                        'metadata': results['metadatas'][0][i] if results['metadatas'] else {},
                        'distance': results['distances'][0][i] if results['distances'] else None,
                        'document': results['documents'][0][i] if results['documents'] else None
                    })

            return formatted_results
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ ChromaDB: {e}")
            return []

    def get_count(self):
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        return self.collection.count()

    def delete_all(self):
        """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        try:
            self.client.delete_collection(self.collection.name)
            print(f"‚úì –ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection.name}' —É–¥–∞–ª–µ–Ω–∞")
            return True
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")
            return False


class CrawlDatabase:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å SQLite –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""

    def __init__(self, db_path='crawl_data.db'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

        Args:
            db_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö SQLite
        """
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self.cursor = self.conn.cursor()
        self._create_tables()
        print(f"‚úì –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥–∫–ª—é—á–µ–Ω–∞: {db_path}")

    def _create_tables(self):
        """–°–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç"""
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS pages (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url TEXT UNIQUE NOT NULL,
                title TEXT,
                content TEXT,
                text_length INTEGER,
                links_count INTEGER,
                crawled_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                metadata TEXT
            )
        ''')

        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_url ON pages(url)
        ''')
        self.cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_crawled_at ON pages(crawled_at)
        ''')

        self.conn.commit()

    def page_exists(self, url):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –≤ –±–∞–∑–µ"""
        self.cursor.execute('SELECT id FROM pages WHERE url = ?', (url,))
        return self.cursor.fetchone() is not None

    def save_page(self, url, title, content, links_count=0, metadata=None):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö

        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            content: –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            links_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (—Å–ª–æ–≤–∞—Ä—å)
        """
        try:
            text_length = len(content)
            metadata_json = json.dumps(metadata) if metadata else None

            self.cursor.execute('''
                INSERT OR REPLACE INTO pages
                (url, title, content, text_length, links_count, metadata, crawled_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (url, title, content, text_length, links_count, metadata_json, datetime.now()))

            self.conn.commit()
            return True
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î: {e}")
            return False

    def get_page(self, url):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ URL"""
        self.cursor.execute('''
            SELECT url, title, content, text_length, links_count, crawled_at, metadata
            FROM pages WHERE url = ?
        ''', (url,))

        row = self.cursor.fetchone()
        if row:
            return {
                'url': row[0],
                'title': row[1],
                'content': row[2],
                'text_length': row[3],
                'links_count': row[4],
                'crawled_at': row[5],
                'metadata': json.loads(row[6]) if row[6] else None
            }
        return None

    def get_all_pages(self, limit=None):
        """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ –±–∞–∑—ã"""
        query = 'SELECT url, title, text_length, crawled_at FROM pages ORDER BY crawled_at DESC'
        if limit:
            query += f' LIMIT {limit}'

        self.cursor.execute(query)
        return self.cursor.fetchall()

    def search_pages(self, search_term):
        """–ü–æ–∏—Å–∫ —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –∏–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–µ"""
        self.cursor.execute('''
            SELECT url, title, text_length, crawled_at
            FROM pages
            WHERE title LIKE ? OR content LIKE ?
            ORDER BY crawled_at DESC
        ''', (f'%{search_term}%', f'%{search_term}%'))

        return self.cursor.fetchall()

    def get_statistics(self):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        self.cursor.execute('SELECT COUNT(*) FROM pages')
        total_pages = self.cursor.fetchone()[0]

        self.cursor.execute('SELECT SUM(text_length) FROM pages')
        total_chars = self.cursor.fetchone()[0] or 0

        self.cursor.execute('SELECT MIN(crawled_at), MAX(crawled_at) FROM pages')
        date_range = self.cursor.fetchone()

        return {
            'total_pages': total_pages,
            'total_characters': total_chars,
            'first_crawl': date_range[0],
            'last_crawl': date_range[1]
        }

    def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
        self.conn.close()
        print("‚úì –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫—Ä—ã—Ç–∞")


class WebCrawler:
    def __init__(self, base_url, max_pages=100, delay=1, stealth_mode=False, auth=None,
                 use_database=True, db_path='crawl_data.db',
                 use_chromadb=False, chroma_collection='crawled_pages'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∞—É–ª–µ—Ä–∞

        Args:
            base_url: –ù–∞—á–∞–ª—å–Ω—ã–π URL –¥–ª—è –∫—Ä–∞—É–ª–∏–Ω–≥–∞
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (None = –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)
            delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø—Ä–∏ stealth_mode –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞)
            stealth_mode: –í–∫–ª—é—á–∏—Ç—å —Å—Ç–µ–ª—Å-—Ä–µ–∂–∏–º (—Å–ª—É—á–∞–π–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏, —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏)
            auth: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
            use_database: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SQLite –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            db_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö SQLite
            use_chromadb: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ChromaDB –¥–ª—è AI-–ø–æ–∏—Å–∫–∞
            chroma_collection: –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ ChromaDB
        """
        self.base_url = base_url
        self.max_pages = max_pages
        self.delay = delay
        self.stealth_mode = stealth_mode
        self.visited_urls = set()
        self.to_visit = [base_url]
        self.results = []
        self.auth = auth
        self.use_database = use_database
        self.use_chromadb = use_chromadb

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SQLite –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        self.db = None
        if use_database:
            self.db = CrawlDatabase(db_path)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB
        self.vector_db = None
        if use_chromadb:
            if not CHROMADB_AVAILABLE:
                print("‚ö†Ô∏è ChromaDB –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ AI-–ø–æ–∏—Å–∫–∞")
                self.use_chromadb = False
            else:
                try:
                    self.vector_db = ChromaVectorDB(collection_name=chroma_collection)
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ChromaDB: {e}")
                    self.use_chromadb = False

        # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è cookies
        self.session = requests.Session()

        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Å—Ç–µ–ª—Å-—Ä–µ–∂–∏–º
        if stealth_mode:
            self._setup_stealth_mode()

        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é
        if auth:
            self._setup_auth(auth)

        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–º–µ–Ω –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏, —á—Ç–æ –æ—Å—Ç–∞–µ–º—Å—è –Ω–∞ —Ç–æ–º –∂–µ —Å–∞–π—Ç–µ
        parsed = urlparse(base_url)
        self.domain = f"{parsed.scheme}://{parsed.netloc}"

    def _setup_stealth_mode(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å—Ç–µ–ª—Å-—Ä–µ–∂–∏–º –¥–ª—è –Ω–µ–∑–∞–º–µ—Ç–Ω–æ–≥–æ –∫—Ä–∞—É–ª–∏–Ω–≥–∞"""
        # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ User-Agent —Å—Ç—Ä–æ–∫–∏ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –±—Ä–∞—É–∑–µ—Ä–æ–≤
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0'
        ]

        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π User-Agent
        self.session.headers.update({
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,ru;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        })

        print(f"‚úì –°—Ç–µ–ª—Å-—Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
        print(f"  User-Agent: {self.session.headers['User-Agent'][:80]}...")
        print(f"  –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞: {self.delay}-{self.delay * 3} —Å–µ–∫")

    def _setup_auth(self, auth):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é"""
        auth_type = auth.get('type')

        if auth_type == 'basic':
            from requests.auth import HTTPBasicAuth
            self.session.auth = HTTPBasicAuth(auth['username'], auth['password'])
            print("‚úì –ù–∞—Å—Ç—Ä–æ–µ–Ω–∞ Basic Auth")

        elif auth_type == 'cookies':
            self.session.cookies.update(auth['cookies'])
            print("‚úì –î–æ–±–∞–≤–ª–µ–Ω—ã cookies")

        elif auth_type == 'headers':
            self.session.headers.update(auth['headers'])
            print("‚úì –î–æ–±–∞–≤–ª–µ–Ω—ã –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")

    def _get_random_delay(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è —Å—Ç–µ–ª—Å-—Ä–µ–∂–∏–º–∞"""
        if self.stealth_mode:
            min_delay = self.delay
            max_delay = self.delay * 3
            return random.uniform(min_delay, max_delay)
        else:
            return self.delay

    def is_valid_url(self, url):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –≤–∞–ª–∏–¥–Ω—ã–π –ª–∏ URL –∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –ª–∏ –æ–Ω —Ç–æ–º—É –∂–µ –¥–æ–º–µ–Ω—É"""
        parsed = urlparse(url)

        if parsed.scheme not in ['http', 'https']:
            return False

        url_domain = f"{parsed.scheme}://{parsed.netloc}"
        if url_domain != self.domain:
            return False

        excluded_extensions = ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.rar', '.doc', '.docx', '.xls', '.xlsx']
        if any(url.lower().endswith(ext) for ext in excluded_extensions):
            return False

        return True

    def extract_links(self, soup, current_url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        links = []
        for link in soup.find_all('a', href=True):
            href = link['href']
            full_url = urljoin(current_url, href)
            full_url = full_url.split('#')[0]

            if self.is_valid_url(full_url):
                links.append(full_url)

        return links

    def crawl_page(self, url):
        """–ö—Ä–∞—É–ª–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        try:
            print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            for script in soup(["script", "style"]):
                script.decompose()

            text = soup.get_text()
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = '\n'.join(chunk for chunk in chunks if chunk)

            title = soup.title.string if soup.title else ''

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∫—Ä–∞—É–ª–∏–Ω–≥–∞
            links = self.extract_links(soup, url)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å
            self.results.append({
                'url': url,
                'title': title,
                'text': text[:1000],
                'text_length': len(text)
            })

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            parsed = urlparse(url)
            metadata = {
                'domain': f"{parsed.scheme}://{parsed.netloc}",
                'path': parsed.path,
                'links_count': len(links)
            }

            if self.db:
                self.db.save_page(url, title, text, len(links), metadata)
                print(f"  üíæ SQLite: –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ (—Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ ChromaDB –¥–ª—è AI-–ø–æ–∏—Å–∫–∞
            if self.vector_db:
                self.vector_db.add_page(url, title, text, metadata)
                print(f"  üß† ChromaDB: Embedding —Å–æ–∑–¥–∞–Ω")

            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –æ—á–µ—Ä–µ–¥—å
            for link in links:
                if link not in self.visited_urls and link not in self.to_visit:
                    self.to_visit.append(link)

            return True

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {str(e)}")
            return False

    def crawl(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∫—Ä–∞—É–ª–∏–Ω–≥–∞"""
        print(f"–ù–∞—á–∏–Ω–∞–µ–º –∫—Ä–∞—É–ª–∏–Ω–≥: {self.base_url}")
        if self.max_pages:
            print(f"–ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü: {self.max_pages}")
        else:
            print(f"–ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü: –ë–ï–ó –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ô ‚ôæÔ∏è")
        print("-" * 80)

        while self.to_visit and (self.max_pages is None or len(self.visited_urls) < self.max_pages):
            url = self.to_visit.pop(0)

            if url in self.visited_urls:
                continue

            self.visited_urls.add(url)
            self.crawl_page(url)

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–ª—É—á–∞–π–Ω–∞—è –≤ —Å—Ç–µ–ª—Å-—Ä–µ–∂–∏–º–µ)
            delay = self._get_random_delay()
            if self.stealth_mode:
                print(f"‚è≥ –ü–∞—É–∑–∞ {delay:.1f} —Å–µ–∫...")
            time.sleep(delay)

        print("-" * 80)
        print(f"–ö—Ä–∞—É–ª–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(self.visited_urls)}")
        print(f"–°–æ–±—Ä–∞–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(self.results)}")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        if self.db:
            stats = self.db.get_statistics()
            print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ SQLite:")
            print(f"  –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {stats['total_pages']}")
            print(f"  –í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {stats['total_characters']:,}")
            if stats['first_crawl']:
                print(f"  –ü–µ—Ä–≤—ã–π –∫—Ä–∞—É–ª–∏–Ω–≥: {stats['first_crawl']}")
                print(f"  –ü–æ—Å–ª–µ–¥–Ω–∏–π –∫—Ä–∞—É–ª–∏–Ω–≥: {stats['last_crawl']}")

        if self.vector_db:
            print(f"\nüß† –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ChromaDB:")
            print(f"  –í—Å–µ–≥–æ embeddings: {self.vector_db.get_count()}")

        return self.results

    def search(self, query, n_results=5):
        """
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —Å–∫—Ä–∞–≤–ª–µ–Ω–Ω—ã–º —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º (—Ç—Ä–µ–±—É–µ—Ç ChromaDB)

        Args:
            query: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –Ω–∞ –ª—é–±–æ–º —è–∑—ã–∫–µ
            n_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏–∑ SQLite
        """
        if not self.vector_db:
            print("‚ö†Ô∏è ChromaDB –Ω–µ –≤–∫–ª—é—á–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ use_chromadb=True –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫—Ä–∞—É–ª–µ—Ä–∞")
            return []

        print(f"\nüîç –ü–æ–∏—Å–∫: '{query}'")
        print("-" * 80)

        # –ò—â–µ–º –≤ ChromaDB
        vector_results = self.vector_db.search(query, n_results)

        if not vector_results:
            print("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return []

        # –î–æ—Å—Ç–∞–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ SQLite
        full_results = []
        for i, result in enumerate(vector_results, 1):
            url = result['url']
            print(f"\n{i}. {result['metadata'].get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')}")
            print(f"   URL: {url}")
            print(f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {1 - result['distance']:.2%}" if result['distance'] else "")

            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ SQLite
            if self.db:
                page_data = self.db.get_page(url)
                if page_data:
                    full_results.append(page_data)
                    print(f"   –¢–µ–∫—Å—Ç–∞: {page_data['text_length']:,} —Å–∏–º–≤–æ–ª–æ–≤")

        return full_results

    def save_results(self, filename='crawl_results.json'):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON —Ñ–∞–π–ª"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")

    def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        if self.db:
            self.db.close()


if __name__ == "__main__":
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫—Ä–∞—É–ª–µ—Ä–∞
    base_url = "https://wiki.gcore.lu/"

    auth_config = {
        'type': 'cookies',
        'cookies': {
            'JSESSIONID': '71F572ACA7F168D486482260A4E7C0E0',
            'croowd.token_key': '_0wiPRNuFIY5vZU7Z1oKngAAAAAAAIACYW50b24uem1paWV2c2t5aUBnY29yZS5sdQ'
        }
    }

    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –∫—Ä–∞—É–ª–µ—Ä
    crawler = WebCrawler(
        base_url=base_url,
        max_pages=None,  # –ë–ï–ó –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ô! –°–∫—Ä–∞–≤–ª–∏—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        delay=2,  # –ë–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ 2 —Å–µ–∫—É–Ω–¥—ã (–≤ —Å—Ç–µ–ª—Å-—Ä–µ–∂–∏–º–µ –±—É–¥–µ—Ç 2-6 —Å–µ–∫)
        stealth_mode=True,  # –í–ö–õ–Æ–ß–ï–ù –°–¢–ï–õ–°-–†–ï–ñ–ò–ú
        auth=auth_config,  # –ü–µ—Ä–µ–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        use_database=True,  # –í–ö–õ–Æ–ß–ï–ù–ê –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö SQLite
        db_path='crawl_data.db',  # –ò–º—è —Ñ–∞–π–ª–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        use_chromadb=True,  # –í–ö–õ–Æ–ß–ï–ù–ê ChromaDB –¥–ª—è AI-–ø–æ–∏—Å–∫–∞
        chroma_collection='wiki_gcore'  # –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ ChromaDB
    )

    try:
        results = crawler.crawl()
        crawler.save_results()

        # –ü—Ä–∏–º–µ—Ä AI-–ø–æ–∏—Å–∫–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å)
        # print("\n" + "="*80)
        # print("–ü–†–ò–ú–ï–† AI-–ü–û–ò–°–ö–ê")
        # print("="*80)
        # search_results = crawler.search("–∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å VPN", n_results=3)
        # if search_results:
        #     print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(search_results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü")

    finally:
        crawler.close()

    # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print("\n–ü—Ä–∏–º–µ—Ä—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü:")
    for i, result in enumerate(results[:5], 1):
        print(f"{i}. {result['title']} - {result['url']}")